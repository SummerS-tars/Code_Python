# 大模型开发实战演示

本演示文件包含了课件14（大模型开发实战）的完整示例代码，展示了如何在Python程序中集成和使用大语言模型。

## 功能演示

### 1. Ollama基础调用

- 基本的chat()函数使用
- messages结构（system/user/assistant角色）
- 系统角色设定对话背景

### 2. 高级功能

- 流式响应（打字机效果）
- 推理过程显示（think参数）
- 多轮对话上下文保持

### 3. 实践案例：问卷自动分析

- 混合处理模式：传统编程 + AI分析
- 单选多选题：Python统计分析
- 开放性文字题：LLM情感分析、主题提取
- 多轮API调用演示

### 4. 伦理与安全

- 数据脱敏处理
- 隐私保护实践
- 幻觉问题应对策略
- 学术诚信提醒

## 环境准备

### 1. 安装Python依赖

```bash
pip install -r requirements.txt
```

### 2. 安装Ollama

1. 访问 [Ollama官网](https://ollama.ai) 下载并安装
2. 拉取推荐模型：

   ```bash
   ollama pull qwen3:0.6b
   ```

3. 启动Ollama服务（安装后会自动启动）

### 3. 验证安装

运行以下命令检查Ollama是否正常工作：

```bash
ollama list
```

## 运行演示

```bash
python llm_integration_demo.py
```

## 注意事项

1. **依赖检查**：程序会自动检查所需依赖，如果缺失会给出安装提示
2. **模型可用性**：确保qwen3:0.6b模型已下载
3. **网络连接**：首次运行可能需要下载模型，请确保网络连接正常
4. **资源占用**：本地模型运行需要一定CPU/内存资源

## 演示内容详解

### 基础调用示例

展示最简单的Ollama chat()函数使用，包括：

- 单轮对话
- 系统角色设定
- 消息格式规范

### 流式响应

演示stream=True参数的使用，实现类似ChatGPT的打字机效果。

### 推理过程

展示模型的思考过程，帮助理解AI的决策逻辑。

### 多轮对话

演示如何维护对话历史，实现有上下文的连续对话。

### 问卷分析案例

完整的实战案例，展示传统编程与AI结合的优势：

- 统计分析：性别、年龄等量化数据
- 情感分析：学习动机的情感倾向判断
- 主题提取：困难点和建议的归纳总结

### 安全实践

- 数据脱敏：保护个人隐私信息
- 伦理提醒：负责任地使用AI技术

## 常见问题

### Ollama连接失败

- 确保Ollama服务正在运行
- 检查防火墙设置
- 尝试重启Ollama服务

### 模型下载失败

- 检查网络连接
- 使用代理（如果在国内网络环境）
- 尝试其他镜像源

### 内存不足

- qwen3:0.6b是轻量级模型，但仍需要一定内存
- 如内存不足，可尝试更小的模型如qwen2:0.5b

## 扩展学习

完成本演示后，可以尝试：

1. 集成其他本地模型（如llama2、mistral）
2. 开发自己的AI应用（如客服机器人、文档分析器）
3. 学习API调用最佳实践和错误处理
4. 探索模型微调和定制化

## 伦理提醒

使用大模型时请注意：

- 🔒 **隐私保护**：不要上传敏感个人信息
- 🎯 **准确性验证**：重要决策需人工复核
- 📚 **学术诚信**：标注AI辅助内容，避免抄袭
- 🤖 **负责任使用**：理解AI局限性，辩证看待结果
